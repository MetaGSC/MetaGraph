{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dataset-Preparation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9ueX0YUedaM",
        "outputId": "e029d3f0-0c64-44fc-d4d6-c17853d14377"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-igraph\n",
            "  Downloading python_igraph-0.9.9-py3-none-any.whl (9.1 kB)\n",
            "Collecting igraph==0.9.9\n",
            "  Downloading igraph-0.9.9-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 13.8 MB/s \n",
            "\u001b[?25hCollecting texttable>=1.6.2\n",
            "  Downloading texttable-1.6.4-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: texttable, igraph, python-igraph\n",
            "Successfully installed igraph-0.9.9 python-igraph-0.9.9 texttable-1.6.4\n",
            "Collecting cairocffi\n",
            "  Downloading cairocffi-1.3.0.tar.gz (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from cairocffi) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.1.0->cairocffi) (2.21)\n",
            "Building wheels for collected packages: cairocffi\n",
            "  Building wheel for cairocffi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cairocffi: filename=cairocffi-1.3.0-py3-none-any.whl size=89668 sha256=2816aa59da9ad8dd0ee75582107acbc5b41f29a12641e8a72f6a26053b354bcf\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/e1/5c8a9692a27f639a07c949044bec943f26c81cd53d3805319f\n",
            "Successfully built cairocffi\n",
            "Installing collected packages: cairocffi\n",
            "Successfully installed cairocffi-1.3.0\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 14.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 14.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 145 kB 13.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 112 kB 66.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 74 kB 2.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 88.6 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install python-igraph\n",
        "!pip install cairocffi\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pysam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ62GCvMeA-s",
        "outputId": "ba8f5305-b6f7-4588-fcd4-bbd6fe7a250b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pysam\n",
            "  Downloading pysam-0.18.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (14.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.9 MB 9.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: pysam\n",
            "Successfully installed pysam-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "kexifZyaetaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gnndatasetpath = \"/content/gdrive/Shareddrives/FYP/DataSets/Gnn-Dataset/Test_Datasets\""
      ],
      "metadata": {
        "id": "JJgfZslxewYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_DEBUG = True"
      ],
      "metadata": {
        "id": "TOhItXqFexIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from igraph import *\n",
        "from collections import defaultdict\n",
        "import os\n",
        "import random\n",
        "\n",
        "%matplotlib inline\n",
        "import torch\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.utils import to_networkx\n",
        "from torch_geometric.nn import GCNConv\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from pysam import FastaFile\n"
      ],
      "metadata": {
        "id": "_syzz_wOfph1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "GwzZ6hwre1jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_graph(G, color):\n",
        "    plt.figure(figsize=(7,7))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    nx.draw_networkx(G, pos=nx.spring_layout(G, seed=42), with_labels=False,\n",
        "                     node_color=color, cmap=\"Set2\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def visualize_embedding(h, color, epoch=None, loss=None):\n",
        "    plt.figure(figsize=(7,7))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    h = h.detach().cpu().numpy()\n",
        "    plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=\"Set2\")\n",
        "    if epoch is not None and loss is not None:\n",
        "        plt.xlabel(f'Epoch: {epoch}, Loss: {loss.item():.4f}', fontsize=16)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "KJKu9QcKfR7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Update the function with the library\n",
        "def predict(sequence):\n",
        "  return [random.uniform(0, 1), random.uniform(0, 1)]"
      ],
      "metadata": {
        "id": "KmfBpKhXfVEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Update the function with the library\n",
        "def get_label_sequences(fastafile):\n",
        "  start = 'NODE_'\n",
        "  end = '_length_'\n",
        "  features = {}\n",
        "  \n",
        "  fasta_object = FastaFile(fastafile)\n",
        "  fasta_references = fasta_object.references\n",
        "\n",
        "  for name in fasta_references:\n",
        "    attributes = {}\n",
        "\n",
        "    contig_num = int(str(int(re.search('%s(.*)%s' % (start, end), name).group(1))-1))\n",
        "    predicted_values = predict(fasta_object.fetch(reference = name))\n",
        "\n",
        "    attributes[\"features\"] = predicted_values\n",
        "    attributes[\"class\"] = 0.5\n",
        "    attributes[\"train_sem\"] = False\n",
        "\n",
        "    # TODO Change this\n",
        "    color_value = sum(predicted_values) / len(predicted_values)\n",
        "\n",
        "    if color_value < 0.4:\n",
        "      attributes[\"class\"] = 0\n",
        "      attributes[\"train_sem\"] = True\n",
        "    elif color_value > 0.8:\n",
        "      attributes[\"class\"] = 1\n",
        "      attributes[\"train_sem\"] = True\n",
        "\n",
        "    features[contig_num] = attributes\n",
        "  return features"
      ],
      "metadata": {
        "id": "FiX8jN7pfSBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_the_segment_contig_map(contigfilepath):\n",
        "  paths = {}\n",
        "  segment_contigs = {}\n",
        "  node_count = 0\n",
        "\n",
        "  with open(contigfilepath) as file:\n",
        "    name = file.readline()\n",
        "    path = file.readline()\n",
        "    \n",
        "    while name != \"\" and path != \"\":\n",
        "            \n",
        "        while \";\" in path:\n",
        "            path = path[:-2]+\",\"+file.readline()\n",
        "        \n",
        "        start = 'NODE_'\n",
        "        end = '_length_'\n",
        "        contig_num = int(str(int(re.search('%s(.*)%s' % (start, end), name).group(1))-1))\n",
        "        \n",
        "        segments = path.rstrip().split(\",\")\n",
        "        \n",
        "        if contig_num not in paths:\n",
        "            node_count += 1\n",
        "            paths[contig_num] = [segments[0], segments[-1]]\n",
        "        \n",
        "        for segment in segments:\n",
        "            # TODO Check this\n",
        "            if segment.endswith(\"+\") or segment.endswith(\"-\"):\n",
        "                segment = segment[:-1]\n",
        "            if segment not in segment_contigs:\n",
        "                segment_contigs[segment] = set([contig_num])\n",
        "            else:\n",
        "                segment_contigs[segment].add(contig_num)\n",
        "        \n",
        "        name = file.readline()\n",
        "        path = file.readline()\n",
        "\n",
        "    return segment_contigs, paths, node_count"
      ],
      "metadata": {
        "id": "WIYrHMxUiOC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_weights(weight_list):\n",
        "  summation = 0\n",
        "  count = 0\n",
        "  none_indexes = []\n",
        "  for i in range(len(weight_list)):\n",
        "    if weight_list[i] == None:\n",
        "      none_indexes.append(i)\n",
        "      continue\n",
        "    count += 1\n",
        "    summation += weight_list[i]\n",
        "  \n",
        "  average_weight = summation / count\n",
        "  for i in none_indexes:\n",
        "    weight_list[i] = average_weight\n",
        "\n",
        "  return weight_list"
      ],
      "metadata": {
        "id": "NvkhYRTA9GBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make segment_contigs global\n",
        "def generate_edge_tensor(gfafilepath, segment_contigs):\n",
        "  source_list = []\n",
        "  destination_list= []\n",
        "  weight_list = []\n",
        "  isNeedToAdjustWeights = False\n",
        "\n",
        "  with open(gfafilepath) as file:\n",
        "    line = file.readline()\n",
        "    while line != \"\":\n",
        "      # Identify lines with link information\n",
        "      if \"L\" in line:\n",
        "          strings = line.split(\"\\t\")\n",
        "          seg1, seg2 = strings[1], strings[3]\n",
        "          weight = strings[5].strip()\n",
        "          contig1 = segment_contigs[seg1]\n",
        "          contig2 = segment_contigs[seg2]\n",
        "          for cont1 in contig1:\n",
        "            for cont2 in contig2:\n",
        "              source_list.append(cont1)\n",
        "              destination_list.append(cont2)\n",
        "              if weight.isnumeric():\n",
        "                weight_list.append(int(weight))\n",
        "              elif weight[:-1].isnumeric():\n",
        "                weight_list.append(int(weight[:-1]))\n",
        "              else:\n",
        "                weight_list.append(None)\n",
        "                isNeedToAdjustWeights = True\n",
        "      line = file.readline()\n",
        "    if isNeedToAdjustWeights:\n",
        "      weight_list = adjust_weights(weight_list)\n",
        "    return source_list, destination_list, weight_list\n"
      ],
      "metadata": {
        "id": "6gFBweMowgnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_tensors(features, source_list, destination_list, weight_list, node_count):\n",
        "  feature_list = []\n",
        "  catergory_list = []\n",
        "  train_data = []\n",
        "  for i in range(node_count):\n",
        "    node_features_dict = features[i]\n",
        "    catergory_list.append(node_features_dict[\"class\"])\n",
        "    train_data.append(node_features_dict[\"train_sem\"])\n",
        "    feature_list.append(node_features_dict[\"features\"])\n",
        "  \n",
        "  node_features = torch.DoubleTensor(feature_list).unsqueeze(1)\n",
        "  y = torch.LongTensor(catergory_list)\n",
        "  print(source_list)\n",
        "  print(destination_list)\n",
        "  print(feature_list)\n",
        "  edge_index = torch.tensor([source_list, destination_list], dtype=torch.long)\n",
        "  edge_attr = torch.DoubleTensor(weight_list)\n",
        "\n",
        "  data = Data(x = node_features, edge_index = edge_index, y = y, edge_attr = edge_attr)\n",
        "  return data"
      ],
      "metadata": {
        "id": "pVZDRykJndqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "z95o2veuKR_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data(gfafilepath, contigfilepath, fastafilepath):\n",
        "  features = get_label_sequences(fastafilepath)\n",
        "  segment_contigs, paths, node_count = get_the_segment_contig_map(contigfilepath)\n",
        "  source_list, destination_list, weight_list = generate_edge_tensor(gfafilepath, segment_contigs)\n",
        "  data = generate_tensors(features, source_list, destination_list, weight_list, node_count)\n",
        "  return data"
      ],
      "metadata": {
        "id": "IcYnDB-AP4L8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process():\n",
        "  directory = gnndatasetpath\n",
        "  data = None\n",
        "  for datafolder in os.listdir(directory):\n",
        "      dataset = os.path.join(directory, datafolder)\n",
        "      gfafilepath = str(dataset) + \"/assembly_graph_with_scaffolds.gfa\"\n",
        "      contigfilepath = str(dataset) + \"/contigs.paths\"\n",
        "      fastafilepath = str(dataset) + \"/contigs.fasta\"\n",
        "\n",
        "      data = generate_data(gfafilepath, contigfilepath, fastafilepath)\n",
        "\n",
        "      G = to_networkx(data, to_undirected=True)\n",
        "      visualize_graph(G, color=data.y)\n",
        "\n",
        "      if _DEBUG:\n",
        "        return data\n",
        "        break"
      ],
      "metadata": {
        "id": "GA_SPds7YIhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = process()"
      ],
      "metadata": {
        "id": "tBfUx4rAXzFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Graph Neural Network Task"
      ],
      "metadata": {
        "id": "zpLzh68Mec6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, inputfeatures = 2, hidden_channels = 2, num_classes = 2):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(1234567)\n",
        "        self.conv1 = GCNConv(inputfeatures, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x"
      ],
      "metadata": {
        "id": "tH8YzLoiec7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "model = GCN(hidden_channels=16)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train():\n",
        "      model.train()\n",
        "      optimizer.zero_grad()  # Clear gradients.\n",
        "      out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
        "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
        "      loss.backward()  # Derive gradients.\n",
        "      optimizer.step()  # Update parameters based on gradients.\n",
        "      return loss\n",
        "\n",
        "def test():\n",
        "      model.eval()\n",
        "      out = model(data.x, data.edge_index)\n",
        "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
        "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
        "      return test_acc\n",
        "\n",
        "def iterate():\n",
        "  for epoch in range(1, 101):\n",
        "      loss = train()\n",
        "      print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
      ],
      "metadata": {
        "id": "l-KpiPaHsiFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCN(inputfeatures = 2, hidden_channels = 2, num_classes = 2)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "gSE5gHFUmLV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "FS_41H-ImLXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.x\n",
        "# data.edge_index"
      ],
      "metadata": {
        "id": "ax5K1QVNntni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = model(data.x, data.edge_index)\n",
        "# visualize(out, color=data.y)"
      ],
      "metadata": {
        "id": "XkBGW7y9mLbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iterate()\n"
      ],
      "metadata": {
        "id": "4tlElrCyec_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # TODO: Update the function with the library\n",
        "# def get_label_sequences(fastafile):\n",
        "#   start = 'NODE_'\n",
        "#   end = '_length_'\n",
        "#   features = {}\n",
        "  \n",
        "#   with open(fastafile, 'r') as file:\n",
        "#     line = file.readline()\n",
        "#     while line != \"\":\n",
        "#       attributes = {}\n",
        "#       if line.startswith(\">\"):\n",
        "\n",
        "#         name = line.lstrip(\">\")\n",
        "#         contig_num = str(int(re.search('%s(.*)%s' % (start, end), name).group(1))-1)\n",
        "#         predicted_values = predict(\"\")\n",
        "\n",
        "#         attributes[\"features\"] = predicted_values\n",
        "#         attributes[\"class\"] = 0.5\n",
        "#         attributes[\"train_sem\"] = False\n",
        "\n",
        "#         # TODO Change this\n",
        "#         color_value = sum(predicted_values) / len(predicted_values)\n",
        "\n",
        "#         if color_value < 0.4:\n",
        "#           attributes[\"class\"] = 0\n",
        "#           attributes[\"train_sem\"] = True\n",
        "#         elif color_value > 0.8:\n",
        "#           attributes[\"class\"] = 1\n",
        "#           attributes[\"train_sem\"] = True\n",
        "\n",
        "#         features[contig_num] = attributes\n",
        "#         line = file.readline()\n",
        "\n",
        "#     return features"
      ],
      "metadata": {
        "id": "gHgkRXV0edBD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}