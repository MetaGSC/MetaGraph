# -*- coding: utf-8 -*-
"""Dataset-Preparation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12VZ0DXvkD7smWXkWQYl89AYhN0XIj95H
"""

# !pip install python-igraph
# !pip install cairocffi



# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html
# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html
# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git

# !pip install pysam

# pip3 install torch==1.9.0 torchvision==0.10.0
# conda install -c bioconda pysam

# pip3 uninstall torch torch-scatter torch-sparse torch-geometric

# pip3 install torch --no-cache-dir
# pip3 install torch-scatter --no-cache-dir -f https://data.pyg.org/whl/torch-1.10.0+cu102.html
# pip3 install torch-sparse --no-cache-dir -f https://data.pyg.org/whl/torch-1.10.0+cu102.html
# pip3 install torch-geometric --no-cache-dir

# from google.colab import drive
# drive.mount('/content/gdrive')

# gnndatasetpath = "/content/gdrive/Shareddrives/FYP/DataSets/Gnn-Dataset/Test_Datasets"
gnndatasetpath = "/home/hp/FYP/GNN/gnn-datasets/Test_Datasets"

_DEBUG = True

# Commented out IPython magic to ensure Python compatibility.
import re
# from igraph import *
from collections import defaultdict
import os
import random

# %matplotlib inline
import torch
import networkx as nx
import matplotlib.pyplot as plt
from torch_geometric.data import Data
from torch_geometric.utils import to_networkx
from torch_geometric.nn import GCNConv

from torch.nn import Linear
import torch.nn.functional as F

from pysam import FastaFile

from sklearn.manifold import TSNE

## Functions


def visualize(h, color, filename = "test_visualize_graph.png"):
    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())

    plt.figure(figsize=(10,10))
    plt.xticks([])
    plt.yticks([])

    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap="Set2")
    plt.show()
    plt.savefig(filename)

def visualize_graph(G, color, filename = "test_visualize_graph.png"):
    plt.figure(figsize=(7,7))
    plt.xticks([])
    plt.yticks([])
    nx.draw_networkx(G, pos=nx.spring_layout(G, seed=42), with_labels=False,
                     node_color=color, cmap="Set2")
    plt.show()
    plt.savefig(filename)


def visualize_embedding(h, color, epoch=None, loss=None, filename = "test_visualize_embedding.png"):
    plt.figure(figsize=(7,7))
    plt.xticks([])
    plt.yticks([])
    h = h.detach().cpu().numpy()
    plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap="Set2")
    if epoch is not None and loss is not None:
        plt.xlabel(f'Epoch: {epoch}, Loss: {loss.item():.4f}', fontsize=16)
    plt.show()
    plt.savefig(filename)

# TODO: Update the function with the library
def predict(sequence):
  return [random.uniform(0, 1), random.uniform(0, 1)]

# TODO: Update the function with the library
def get_label_sequences(fastafile):
  start = 'NODE_'
  end = '_length_'
  features = {}
  
  fasta_object = FastaFile(fastafile)
  fasta_references = fasta_object.references

  for name in fasta_references:
    attributes = {}

    contig_num = int(str(int(re.search('%s(.*)%s' % (start, end), name).group(1))-1))
    predicted_values = predict(fasta_object.fetch(reference = name))

    attributes["features"] = predicted_values
    attributes["class"] = 0.5
    attributes["train_sem"] = False

    # TODO Change this
    color_value = sum(predicted_values) / len(predicted_values)

    if color_value < 0.4:
      attributes["class"] = 0
      attributes["train_sem"] = True
    elif color_value > 0.8:
      attributes["class"] = 1
      attributes["train_sem"] = True

    features[contig_num] = attributes
  return features

def get_the_segment_contig_map(contigfilepath):
  paths = {}
  segment_contigs = {}
  node_count = 0

  with open(contigfilepath) as file:
    name = file.readline()
    path = file.readline()
    
    while name != "" and path != "":
            
        while ";" in path:
            path = path[:-2]+","+file.readline()
        
        start = 'NODE_'
        end = '_length_'
        contig_num = int(str(int(re.search('%s(.*)%s' % (start, end), name).group(1))-1))
        
        segments = path.rstrip().split(",")
        
        if contig_num not in paths:
            node_count += 1
            paths[contig_num] = [segments[0], segments[-1]]
        
        for segment in segments:
            # TODO Check this
            if segment.endswith("+") or segment.endswith("-"):
                segment = segment[:-1]
            if segment not in segment_contigs:
                segment_contigs[segment] = set([contig_num])
            else:
                segment_contigs[segment].add(contig_num)
        
        name = file.readline()
        path = file.readline()

    return segment_contigs, paths, node_count

def adjust_weights(weight_list):
  summation = 0
  count = 0
  none_indexes = []
  for i in range(len(weight_list)):
    if weight_list[i] == None:
      none_indexes.append(i)
      continue
    count += 1
    summation += weight_list[i][0]
  
  average_weight = summation / count
  for i in none_indexes:
    weight_list[i] = [average_weight]

  return weight_list

#make segment_contigs global
def generate_edge_tensor(gfafilepath, segment_contigs):
  source_list = []
  destination_list= []
  weight_list = []
  isNeedToAdjustWeights = False

  with open(gfafilepath) as file:
    line = file.readline()
    while line != "":
      # Identify lines with link information
      if "L" in line:
          strings = line.split("\t")
          seg1, seg2 = strings[1], strings[3]
          weight = strings[5].strip()
          contig1 = segment_contigs[seg1]
          contig2 = segment_contigs[seg2]
          for cont1 in contig1:
            for cont2 in contig2:
              source_list.append(cont1)
              destination_list.append(cont2)
              if weight.isnumeric():
                weight_list.append([int(weight)])
              elif weight[:-1].isnumeric():
                weight_list.append([int(weight[:-1])])
              else:
                weight_list.append(None)
                isNeedToAdjustWeights = True
      line = file.readline()
    if isNeedToAdjustWeights:
      weight_list = adjust_weights(weight_list)
    return source_list, destination_list, weight_list


def generate_tensors(features, source_list, destination_list, weight_list, node_count):
  feature_list = []
  catergory_list = []
  train_data = []
  test_data = []
  for i in range(node_count):
    node_features_dict = features[i]
    catergory_list.append(node_features_dict["class"])
    train_data.append(node_features_dict["train_sem"])
    test_data.append(not (node_features_dict["train_sem"]))
    feature_list.append(node_features_dict["features"])
  
  train_tensor = torch.BoolTensor(train_data)
  test_tensor = torch.BoolTensor(test_data)
  
  # node_features = torch.DoubleTensor(feature_list).unsqueeze(1)
  node_features = torch.DoubleTensor(feature_list)
  y = torch.LongTensor(catergory_list)
  # print(source_list)
  # print(destination_list)
  # print(feature_list)
  edge_index = torch.tensor([source_list, destination_list], dtype=torch.long)
  edge_attr = torch.DoubleTensor(weight_list)

  # data = Data(x = node_features, edge_index = edge_index, y = y, edge_attr = edge_attr)
  data = Data(x = node_features, edge_index = edge_index, y = y)
  return data, train_tensor, test_tensor



def generate_data(gfafilepath, contigfilepath, fastafilepath):
  features = get_label_sequences(fastafilepath)
  segment_contigs, paths, node_count = get_the_segment_contig_map(contigfilepath)
  source_list, destination_list, weight_list = generate_edge_tensor(gfafilepath, segment_contigs)
  data, train_data, test_data = generate_tensors(features, source_list, destination_list, weight_list, node_count)
  return data, train_data, test_data

### Graph Neural Network Task

class GCN(torch.nn.Module):
    def __init__(self, inputfeatures = 2, hidden_channels = 2, num_classes = 2):
        super().__init__()
        torch.manual_seed(1234567)
        self.conv1 = GCNConv(inputfeatures, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, num_classes)
        self.classifier = Linear(num_classes, num_classes)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = x.relu()
        x = F.dropout(x, p=0.5, training=self.training)
        x = self.conv2(x, edge_index)

        # Apply a final (linear) classifier.
        out = self.classifier(x)
        return out, x

def train():
      model.train()
      optimizer.zero_grad()  # Clear gradients.
      out, h = model(data.x, data.edge_index)  # Perform a single forward pass.
      loss = criterion(out[train_data], data.y[train_data])  # Compute the loss solely based on the training nodes.
      loss.backward()  # Derive gradients.
      optimizer.step()  # Update parameters based on gradients.
      return loss, h

def test():
      model.eval()
      out, h = model(data.x, data.edge_index)
      pred = out.argmax(dim=1)  # Use the class with highest probability.
      # test_correct = pred[test_data] == data.y[test_data]  # Check against ground-truth labels.
      test_correct = pred[train_data] == data.y[train_data]  # Check against ground-truth labels.
      test_acc = int(test_correct.sum()) / int(test_data.sum())  # Derive ratio of correct predictions.
      return test_acc, h

def iterate():
  for epoch in range(1, 101):
      loss = train()
      print(f'Epoch: ', epoch, "Loss: ", loss)

def process():
  directory = gnndatasetpath
  data = None
  for datafolder in os.listdir(directory):
      dataset = os.path.join(directory, datafolder)
      print(dataset, "\n")
      gfafilepath = str(dataset) + "/assembly_graph_with_scaffolds.gfa"
      contigfilepath = str(dataset) + "/contigs.paths"
      fastafilepath = str(dataset) + "/contigs.fasta"

      data, train_data, test_data = generate_data(gfafilepath, contigfilepath, fastafilepath)

      G = to_networkx(data, to_undirected=True)
      visualize_graph(G, color=data.y, filename = "Figures/test_visualize_graph.png")

      if _DEBUG:
        return data, train_data, test_data
        break

data, train_data, test_data = process()

# from IPython.display import Javascript  # Restrict height of output cell.
# display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))

model = GCN(inputfeatures = 2, hidden_channels = 2, num_classes = 2)
model.double()
print(model)

eval_results = model.eval()
print(eval_results)

# print("x:- ", len(data.x))
# data.edge_index

out, h = model(data.x, data.edge_index)
print(f'Embedding shape: {list(h.shape)}')

visualize_embedding(h, color=data.y, filename = "Figures/test_visualize_embedding.png")
visualize(h, color=data.y, filename = "Figures/test_visualize.png")

optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)
criterion = torch.nn.CrossEntropyLoss()

iterate()

test_acc = test()
print("Test Accuracy: ", test_acc)

model.eval()

out, h = model(data.x, data.edge_index)
visualize(h, color=data.y, filename = "Figures/test_results_visualize_embedding.png")

# # TODO: Update the function with the library
# def get_label_sequences(fastafile):
#   start = 'NODE_'
#   end = '_length_'
#   features = {}
  
#   with open(fastafile, 'r') as file:
#     line = file.readline()
#     while line != "":
#       attributes = {}
#       if line.startswith(">"):

#         name = line.lstrip(">")
#         contig_num = str(int(re.search('%s(.*)%s' % (start, end), name).group(1))-1)
#         predicted_values = predict("")

#         attributes["features"] = predicted_values
#         attributes["class"] = 0.5
#         attributes["train_sem"] = False

#         # TODO Change this
#         color_value = sum(predicted_values) / len(predicted_values)

#         if color_value < 0.4:
#           attributes["class"] = 0
#           attributes["train_sem"] = True
#         elif color_value > 0.8:
#           attributes["class"] = 1
#           attributes["train_sem"] = True

#         features[contig_num] = attributes
#         line = file.readline()

#     return features